import h5py as h5py
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
import platform
from math import log10
import csv
import shutil
import scipy.misc
if 'cdr' in platform.node() or 'ts' in platform.node():
    import matplotlib.pyplot
    matplotlib.pyplot.switch_backend('agg')
from skimage.measure import compare_ssim
from matplotlib import pylab
import matplotlib.pyplot as plt
import os
import numpy.ma as ma
import pickle as pickle
import numpy as np
from skimage.transform import resize
import itertools
import torch.nn as nn
import copy
import logging
import modules.pytorch_msssim as pyt_ssim

try:
    logger = logging.getLogger(__file__.split('/')[-1])
except:
    logger = logging.getLogger(__name__)


def show_volume(sample, slice_idx=3, modality=1):
    """
    params:
        sample: Sample dictionary generated by the DataLoader class
        slice_idx: The dimension along which to index the 3D volume for display[1,3]
        modality: Index of modality to display [0,4]
    """
    data = np.array(sample['image']).transpose(1, 2, 3, 0)
    pat_name = sample['name']
    seg = np.array(sample['seg']).transpose(1, 2, 0)
    total_slices = data.shape[slice_idx]
    offset = total_slices // 10
    fig, ax = plt.subplots(nrows=2, ncols=5, squeeze=False, figsize=(20, 10))
    ax = [i for ls in ax for i in ls]
    c = 0
    for a in ax:
        if c < total_slices:
            if slice_idx == 1:
                image_mask = ma.masked_where(seg > 0, seg)
                a.imshow(data[modality, c, :, :], cmap='gray')
                a.imshow(image_mask[c, :, :], cmap='rainbow', alpha=0.4)
            elif slice_idx == 2:
                image_mask = ma.masked_where(seg > 0, seg)
                a.imshow(data[modality, :, c, :], cmap='gray')
                a.imshow(image_mask[:, c, :], cmap='rainbow', alpha=0.4)
            elif slice_idx == 3:
                image_mask = ma.masked_where(seg > 0, seg)
                a.imshow(data[modality, :, :, c], cmap='gray')
                a.imshow(image_mask[:, :, c], cmap='rainbow', alpha=0.4)
            a.axis('off')
            c += offset
        else:
            break
    plt.tight_layout()
    plt.suptitle(pat_name)
    plt.show()


class BrainMRIData(Dataset):
    def __init__(self, h5path, mean_var_path, parent_name, dataset_name,
                 load_pat_names, load_seg,
                 transform=None, apply_normalization=False, which_normalization=None,
                 dataset='BRATS2018', load_indices=None, train_range=None):
        """
        params:
            h5path: Path to the HDF5 file from which to import data
            mean_var_path: Path to the mean/variance file for the HDf5
                           file
            parent_name: Parent name of the group in HDF5 file
            dataset_name: Name of dataset in HDF5 file to import data
                          from.
            load_pat_names: Boolean. Whether or not to load patient
                            names
            load_seg: Whether or not to load segmentations
            transform: Whether to apply a transformation on data
        """
        self.valid_parent_names = ['original_data',
                                   'combined',
                                   'preprocessed']

        self.valid_dataset_names = ['training_data_hgg',
                                    'training_data_hgg_pat_name',
                                    'training_data_lgg',
                                    'training_data_lgg_pat_name',
                                    'training_data_segmasks_hgg',
                                    'training_data_segmasks_lgg',
                                    'validation_data',
                                    'validation_data_pat_name',
                                    # for combined h5py file
                                    'training_data',
                                    'training_data_pat_name',
                                    'training_data_segmasks']

        if h5path is None:
            raise ValueError("Please specify the path for the HDF5 file")

        if mean_var_path is None:
            raise ValueError("Please specify the path for the mean/var file")

        if parent_name is None or \
                parent_name not in self.valid_parent_names:
            raise ValueError("Invalid parent group name, please check")

        if dataset_name is None or \
                dataset_name not in self.valid_dataset_names:
            raise ValueError("Invalid dataset name, please check")

        self.h5path = h5path
        self.h5file = h5py.File(h5path, 'r')
        # file was pickled in Python 2, so to open in Python 3
        # we need to set encoding
        self.mean_var_file = pickle.load(open(mean_var_path, 'rb'),
                                         encoding='latin1')
        self.parent_name = parent_name

        # the actual dataset that we load
        self.xdataset_name = dataset_name
        # corresponding segmentation mask dataset
        self.ydataset_name = dataset_name + '_segmasks'
        # corresponding name dataset
        self.name_dataset_name = dataset_name + "_pat_name"
        self.apply_normalization = apply_normalization

        self.dataset = dataset
        self.load_indices = load_indices

        try:
            logger.info("Loading datasets to memory to make the training faster")
            self.xdataset = self.h5file[self.parent_name][self.xdataset_name]
            self.x_max = []
            if train_range is not None:
                # only calculate max value from the training_range, not from testing patients
                # Problem is, three patients have an error intensity value of 32767.0 as max, which messes up
                # normalization. Hence find those patients, and remove them from intensity max calculation
                # curr_max = {0: 0, 1: 0, 2: 0, 3: 0}
                # for i in train_range:
                #     for m in range(0, 4):
                #         if (not self.xdataset[i, m].max() > 30000.0) and (self.xdataset[i, m].max() >= curr_max[m]):
                #             curr_max[m] = self.xdataset[i, m].max()

                # typically BRATS uses 12 bits (all dicoms do), but there  are like
                self.x_max = {0: 4096.0,  1: 4096.0, 2: 4096.0, 3: 4096.0}

            if 'validation_data' not in self.xdataset_name and load_seg:
                self.ydataset = self.h5file[self.parent_name][self.ydataset_name]
            self.name_dataset = self.h5file[self.parent_name][self.name_dataset_name]
        except KeyError as e:
            text = 'Attempted keys: {}, {}, {}'.format(self.xdataset_name,
                                                       self.ydataset_name,
                                                       self.name_dataset)
            raise KeyError(text)

        self.load_pat_names = load_pat_names
        self.load_seg = load_seg
        self.transform = transform

        if self.apply_normalization:
            if self.dataset == 'BRATS2018' or self.dataset == 'BRATS2015':
                if which_normalization is None:
                    # self.normalization_function = self.apply_mean_std
                    self.normalization_function = self.apply_normalization_for_ISLES
                elif which_normalization == 'tanh':
                    print('Using TANH normalization')
                    self.normalization_function = self.apply_tanh_normalization
                # self.normalization_function = self.apply_mean_std
            elif self.dataset == 'ISLES2015':
                self.normalization_function = self.apply_normalization_for_ISLES

    def __len__(self):
        try:
            return len(self.h5file[self.parent_name][self.xdataset_name])
        except ValueError:
            return 0

    def getitem_via_index(self, idx):
        sample = {}

        if self.apply_normalization:
            # if its ISLES2015 normalization, it will ignore the mean_var_file. Otherwise it uses it.
            sample['image'] = self.normalization_function(self.xdataset[idx], self.mean_var_file)
        else:
            sample['image'] = self.xdataset[idx]

        if self.load_seg:
            sample['seg'] = self.ydataset[idx]

        if self.load_pat_names:
            sample['name'] = self.name_dataset[idx]

        if self.transform:
            sample = self.transform(sample)

        return sample

    def __getitem__(self, idx):
        sample = {}

        if self.load_indices is not None:
            # convert the idx into the relative index in load_indices train/test index generated for cross validation
            idx = self.load_indices[idx]

        logger.debug("value of idx in dataloader's __getitem__: {}".format(idx))
        if self.apply_normalization:
            # if its ISLES2015 normalization, it will ignore the mean_var_file. Otherwise it uses it.
            sample['image'] = self.normalization_function(self.xdataset[idx], self.mean_var_file)
        else:
            sample['image'] = self.xdataset[idx]

        if self.load_seg:
            sample['seg'] = self.ydataset[idx]

        if self.load_pat_names:
            sample['name'] = self.name_dataset[idx]

        if self.transform:
            sample = self.transform(sample)

        return sample

    def apply_normalization_for_ISLES(self, im, mean_var_file):
        """
        In order to not break compatibility with anothe rnormaization function for BRATS, we have an unused argument here.
        :param im:
        :param mean_var_file:
        :return:
        """
        # remove all negative values
        im[im < 0] = 0.0
        for m in range(0, 4):
            if len(im.shape) > 4:
                for k in range(0, im.shape[0]):
                    im[k, m, ...] = (im[k, m, ...] / np.mean(im[k, m, ...]))
            else:
                # remove clipping
                # if im[m, ...].min() != 0.0:
                #     im[m, ...] = np.clip(im[m, ...], a_min=0.0, a_max=np.max(im[m, ...]))

                im[m, ...] = (im[m, ...] / np.mean(im[m, ...]))

        return im

    def apply_tanh_normalization(self, im, max_val=None):
        im = self.apply_mean_std(im, self.mean_var_file)
        # make sure there are no negative entries
        # im[im < 0] = 0.0

        for m in range(0, 4):
            if len(im.shape) > 4:
                for k in range(0, im.shape[0]):
                    den = self.x_max[m] - im[k, m, ...].min() # max of this modality across the dataset
                    im[k, m, ...] = 2 * ((im[k, m, ...] - im[k, m, ...].min()) / den) - 1
            else:
                den = im[m, ...].max() - im[m, ...].min()  # max of this modality across the dataset
                im[m, ...] = 2 * ((im[m, ...] - im[m, ...].min()) / den) - 1

        final = np.clip(im, -1, 1)
        return final

    def apply_tanh_normalization_old(self, im, dummy):
        # make sure there are no negative entries
        im[im < 0] = 0.0

        for m in range(0, 4):
            if len(im.shape) > 4:
                for k in range(0, im.shape[0]):
                    den = im[k, m, ...].max() - im[k, m, ...].min()
                    if den == 0.0:
                        im[k, m, ...] = -1.0
                    else:
                        im[k, m, ...] = 2 * ((im[k, m, ...] - im[k, m, ...].min()) /  den) - 1
            else:
                den = im[m, ...].min() - im[m, ...].min()

                if den == 0.0:
                    im[m, ...] = -1.0
                else:
                    im[m, ...] = 2 * ((im[m, ...] - im[m, ...].min()) / den) - 1
        return im

    def apply_mean_std(self, im, mean_var):
        """
        Supercedes the standardize function. Takes the mean/var  file generated during preprocessed data generation and
        applies the normalization step to the patch.
        :param im: patch of size  (num_egs, channels, x, y, z) or (channels, x, y, z)
        :param mean_var: dictionary containing mean/var value calculated in preprocess.py
        :return: normalized patch
        """

        # expects a dictionary of means and VARIANCES, NOT STD
        for m in range(0, 4):
            if len(im.shape) > 4:
                im[:, m, ...] = (im[:, m, ...] - mean_var['mn'][m]) / np.sqrt(mean_var['var'][m])
            else:
                im[m, ...] = (im[m, ...] - mean_var['mn'][m]) / np.sqrt(mean_var['var'][m])

        return im

    def close_connection(self):
        self.h5file.close()

    def reconnect(self):
        try:
            self.h5file = h5py.File(self.h5path, 'r')
            return True
        except:
            raise

def revert_mean_std(im, mean_var):
    """
    Inverse function of apply_mean_std to get back original intensities
    :param im:
    :param mean_var:
    :return:
    """
    for m in range(0, 4):
        if len(im.shape) > 4:
            im[:, m, ...] = (im[:, m, ...] * np.sqrt(mean_var['var'][m]) + mean_var['mn'][m])
        else:
            im[m, ...] = (im[m, ...] * np.sqrt(mean_var['var'][m]) + mean_var['mn'][m])

    return im


class ToTensor(object):
    """Convert ndarrays in sample to Tensors."""

    def __call__(self, sample):
        image = sample['image']
        seg = sample.get('seg')
        name = sample.get('name')
        final = {}

        # swap color axis because
        # numpy image: H x W x Z x C
        # torch image: C X H X W x Z
        final['image'] = image.transpose((3, 0, 1, 2))
        final['image'] = torch.from_numpy(final['image']).type(torch.FloatTensor)

        if seg is not None:
            final['seg'] = seg.transpose((2, 0, 1))
            final['seg'] = torch.from_numpy(final['seg'])

        if name is not None:
            final['name'] = name

        return final


class Resize(object):
    """Convert ndarrays in sample to Tensors."""

    def __init__(self, size):
        self.size = size

    def __call__(self, sample):
        image = sample['image']
        seg = sample.get('seg')
        name = sample.get('name')
        final = {}
        sh = image.shape
        size = self.size

        dummy_im = np.empty((sh[0], size[0], size[1], sh[-1]))

        for curr_slice in range(0, image.shape[-1]):
            for curr_seq in range(0, 4):
                dummy_im[curr_seq, :, :, curr_slice] = resize(image[curr_seq, :, :, curr_slice], output_shape=size,
                                                              preserve_range=True)

        sample['image'] = dummy_im

        return sample


def my_collate(batch):
    image_arrays = (curr_batch['image'] for curr_batch in batch)
    seg_arrays = (curr_batch['seg'] for curr_batch in batch if 'seg' in curr_batch)
    name_arrays = (curr_batch['name'] for curr_batch in batch if 'name' in curr_batch)

    final_batch = {}
    collated_image_array = torch.cat(tuple(image_arrays), dim=0)
    final_batch['image'] = collated_image_array

    if 'seg' in batch[0].keys():
        collated_seg_array = torch.cat(tuple(seg_arrays), dim=0)
        final_batch['seg'] = collated_seg_array

    if 'name' in batch[0].keys():
        collated_name_array = np.vstack(name_arrays)
        final_batch['name'] = collated_name_array

    return final_batch


def generate_training_strategy(dataset_name, curr_epoch, total_epochs):
    if dataset_name == 'ISLES2015':
        FIRST_BRACKET = 50 # X% of the epochs for easy scenarios
        if curr_epoch <= ((total_epochs * FIRST_BRACKET) / 100):
            logger.debug('First Bracket')
            rand_val = torch.randint(low=3, high=7, size=(1,))

        SECOND_BRACKET = 80
        # if the curr_epoch is above X%%, but less than T%, train ONLY with difficult examples
        if (curr_epoch > ((total_epochs * FIRST_BRACKET) / 100)) and \
                (curr_epoch <= ((total_epochs * SECOND_BRACKET) / 100)):
            logger.debug('Second Bracket')
            rand_val = torch.randint(low=0, high=3, size=(1,))

        if (curr_epoch > ((total_epochs * SECOND_BRACKET) / 100)):
            logger.debug('Third Bracket')
            rand_val = torch.randint(low=0, high=7, size=(1,))

    elif dataset_name == 'BRATS2018':
        FIRST_BRACKET = 30  # X% of the epochs for easy scenarios
        if curr_epoch <= ((total_epochs * FIRST_BRACKET) / 100):
            logger.debug('First Bracket')
            rand_val = torch.randint(low=10, high=14, size=(1,))

        SECOND_BRACKET = 70
        # if the curr_epoch is above X%%, but less than T%, train ONLY with difficult examples
        if (curr_epoch > ((total_epochs * FIRST_BRACKET) / 100)) and \
                (curr_epoch <= ((total_epochs * SECOND_BRACKET) / 100)):
            logger.debug('Second Bracket')
            rand_val = torch.randint(low=4, high=10, size=(1,))

        THIRD_BRACKET = 90
        if (curr_epoch > ((total_epochs * SECOND_BRACKET) / 100)) and \
                (curr_epoch <= ((total_epochs * THIRD_BRACKET) / 100)):
            logger.debug('Third Bracket')
            rand_val = torch.randint(low=0, high=4, size=(1,))

        if (curr_epoch > ((total_epochs * THIRD_BRACKET) / 100)):
            logger.debug('Third Bracket')
            rand_val = torch.randint(low=0, high=14, size=(1,))

    return rand_val

def show_intermediate_results_BRATS(G, test_patient, save_path, all_scenarios, epoch, curr_scenario_range=None, batch_size_to_test=5):
    if isinstance(test_patient, list):
        patients = test_patient
    else:
        patients = [test_patient]
    # put the generator in EVAL mode.
    G.eval()
    mse = nn.MSELoss()
    # mse_total_1_missing = []
    # mse_total_2_missing = []
    # mse_total_3_missing = []
    #
    # psnr_total_1_missing = []
    # psnr_total_2_missing = []
    # psnr_total_3_missing = []
    #
    # ssim_total_1_missing = []
    # ssim_total_2_missing = []
    # ssim_total_3_missing = []

    sh = patients[0]['image'].shape
    G_all_slices = np.empty((sh[0], sh[1], sh[2], sh[3]))

    for patient in patients:
        pat_name = patient['name'].decode('UTF-8')
        logger.info('Testing Patient: {}'.format(pat_name))

        curr_saving_directory = os.path.join(save_path, pat_name, "epoch_" + str(epoch))

        if not os.path.isdir(curr_saving_directory):
            os.makedirs(curr_saving_directory)

        patient_copy = patient['image'].clone()
        patient_numpy = patient_copy.detach().cpu().numpy()
        scenarios = copy.deepcopy(all_scenarios)

        sh = patient_numpy.shape

        batch_size = batch_size_to_test

        if curr_scenario_range is not None:
            scenarios = scenarios[curr_scenario_range[0]:curr_scenario_range[1]]

        logger.info('Testing on scenarios: {}'.format(scenarios))
        # what's the current scenario
        for curr_scenario in scenarios:

            logger.info('Testing on scenario: {}'.format(curr_scenario))
            # get the batch indices
            batch_indices = range(0, sh[0], batch_size)

            G_mse_loss = []
            G_ssim_val = []
            G_psnr_val = []
            # for each batch
            for _num, batch_idx in enumerate(batch_indices):
                x_test_r = patient['image'][batch_idx:batch_idx + batch_size, ...].cuda()
                x_test_z = x_test_r.clone().cuda().type(torch.cuda.FloatTensor)

                for idx_, k in enumerate(curr_scenario):
                    if k == 0:
                        x_test_z[:, idx_, ...] = torch.randn((sh[-2], sh[-1]))

                G_result = G(x_test_z)

                for idx_curr_label, i in enumerate(curr_scenario):
                    if i == 0:
                        G_mse_loss.append(mse(
                            G_result[:, idx_curr_label, ...],
                            x_test_r[:, idx_curr_label, ...]).item())
                        G_ssim_val.append(pyt_ssim.ssim(
                            G_result[:, idx_curr_label, ...].unsqueeze(1),
                            x_test_r[:, idx_curr_label, ...].unsqueeze(1),
                            val_range=2).item())
                        G_psnr_val.append(psnr_torch(
                            G_result[:, idx_curr_label, ...].unsqueeze(1),
                            x_test_r[:, idx_curr_label, ...].unsqueeze(1),
                            val_range=2).item())

                g_out_np = G_result.detach().cpu().numpy()
                G_all_slices[batch_idx:batch_idx + batch_size] = g_out_np

            slice_offset = 10
            start_slice = 0
            end_slice = 150 if sh[0] >= 150 else sh[0]
            num_synthesized = curr_scenario.count(0)
            rows = 4 + num_synthesized
            cols = (end_slice - start_slice) // slice_offset

            f, axes = plt.subplots(rows, cols, sharey=True, figsize=(30, 10))

            # plot the original real patient, row by row until 4th row
            # the order of sequences in HDF5 is as follows: (Not T1, T1c, T2, T2F).
            seq_names = ['T1', 'T2', 'T1c', 'T2F']

            for curr_row in range(0, 4):  # curr_row also controls the sequence to show
                for curr_col, curr_slice_idx in zip(range(0, cols), range(start_slice, end_slice, slice_offset)):
                    # we use patient['image'] as it contains ALL slices for the patient, not batch wise
                    axes[curr_row, curr_col].imshow(patient_numpy[curr_slice_idx, curr_row, ...], cmap='gray')
                    axes[curr_row, curr_col].axis('off')
                axes[curr_row, 0].axis('on')
                axes[curr_row, 0].get_xaxis().set_ticks([])
                axes[curr_row, 0].get_yaxis().set_ticks([])
                axes[curr_row, 0].set_ylabel(seq_names[curr_row], rotation=90, size='large')

            # now we plot the synthesized one
            indices = [i for i, x in enumerate(curr_scenario) if x == 0]
            for curr_row, curr_sequence_index in zip(range(4, rows),
                                                     indices):  # curr_row also controls the sequence to show
                for curr_col, curr_slice_idx in zip(range(0, cols), range(start_slice, end_slice, slice_offset)):
                    # we use patient['image'] as it contains ALL slices for the patient, not batch wise
                    axes[curr_row, curr_col].imshow(G_all_slices[curr_slice_idx, curr_sequence_index,...], cmap='gray')
                    axes[curr_row, curr_col].axis('off')
                axes[curr_row, 0].axis('on')
                axes[curr_row, 0].get_xaxis().set_ticks([])
                axes[curr_row, 0].get_yaxis().set_ticks([])
                axes[curr_row, 0].set_ylabel(seq_names[curr_sequence_index], rotation=90, size='large')

            # Question: Why is the ordering like this?
            # Answer: Check the original dataloader in dataloader.py. The "i" values are this way.
            plt.suptitle('Epoch: {}\nT1   T2   T1c   T2F\n{}   {}   {}   {}'.format(epoch,
                                                                                      curr_scenario[0],
                                                                                     curr_scenario[1],
                                                                                     curr_scenario[2],
                                                                                     curr_scenario[3]))

            f.text(0.65, 0.95, pat_name)
            f.text(0.35, 0.95, "MSE: %.5f" % np.mean(G_mse_loss))
            f.text(0.35, 0.935, "PSNR: %.5f" % np.mean(G_psnr_val))
            f.text(0.35, 0.92, "SSIM: %.5f" % np.mean(G_ssim_val))

            plt.savefig(os.path.join(curr_saving_directory, "".join([str(x) for x in curr_scenario]) + ".png"))
            pylab.close(f)
        del patient_numpy, patient_copy, patient
            # plt.savefig(result_pathname + "_" + "{}".format(pat_name) + "_" + "".join([str(x) for x in curr_scenario]) + ".png", bbox_inches='tight')

    # put the generator back to train mode
    G.train()
    return 0


def show_intermediate_results(G, test_patient, save_path, all_scenarios, epoch, curr_scenario_range=None,
                              batch_size_to_test=5, seq_type='T1', dataset='ISLES2015'):

    if isinstance(test_patient, list):
        patients = test_patient
    else:
        patients = [test_patient]
    # put the generator in EVAL mode.
    mse = nn.MSELoss()
    # mse_total_1_missing = []
    # mse_total_2_missing = []
    # mse_total_3_missing = []
    #
    # psnr_total_1_missing = []
    # psnr_total_2_missing = []
    # psnr_total_3_missing = []
    #
    # ssim_total_1_missing = []
    # ssim_total_2_missing = []
    # ssim_total_3_missing = []

    sh = patients[0]['image'].shape
    G_all_slices = np.empty((sh[0], 1, sh[-2], sh[-1]))

    for patient in patients:
        pat_name = patient['name'].decode('UTF-8')
        logger.debug('Testing Patient: {}'.format(pat_name))

        curr_saving_directory = os.path.join(save_path, pat_name, "epoch_" + str(epoch))

        if not os.path.isdir(curr_saving_directory):
            os.makedirs(curr_saving_directory)

        patient_copy = patient['image'].clone()
        patient_numpy = patient_copy.detach().cpu().numpy()
        scenarios = all_scenarios

        if seq_type == "T1":
            SEQ_IDX = 0
        elif seq_type == 'T2':
            SEQ_IDX = 1
        else: # this is for ISLES2015
            SEQ_IDX = 3

        sh = patient_numpy.shape

        batch_size = batch_size_to_test

        if curr_scenario_range is not None:
            scenarios = scenarios[curr_scenario_range[0]:curr_scenario_range[1]]

        logger.debug('Testing on scenarios: {}'.format(scenarios))
        # what's the current scenario
        for curr_scenario in scenarios:

            logger.debug('Testing on scenario: {}'.format(curr_scenario))
            # get the batch indices
            batch_indices = range(0, sh[0], batch_size)

            # for each batch
            G_mse_loss = []
            G_ssim_val = []
            G_psnr_val = []
            for _num, batch_idx in enumerate(batch_indices):
                x_test_r = patient['image'][batch_idx:batch_idx + batch_size, ...].cuda()
                x_test_z = x_test_r.clone().cuda().type(torch.cuda.FloatTensor)

                for idx_, k in enumerate(curr_scenario):
                    if k == 0:
                        x_test_z[:, idx_, ...] = torch.ones((sh[-2], sh[-1])) * -1.0

                G_result = G(x_test_z)

                for i in range(batch_size):
                    G_mse_loss.append(mse(G_result[i, 0, ...], x_test_r[i, SEQ_IDX, ...]).item())

                G_ssim_val.append(pyt_ssim.ssim(
                    G_result[:, 0].unsqueeze(1) / (torch.max(x_test_r[:, SEQ_IDX]) + 0.0001),
                    x_test_r[:, SEQ_IDX].unsqueeze(1) / (torch.max(x_test_r[:, SEQ_IDX]) + 0.0001),
                    val_range=1).item())

                G_psnr_val.append(
                    psnr_torch(G_result, x_test_r[:, SEQ_IDX, ...].unsqueeze(1)).item())

                g_out_np = G_result.detach().cpu().numpy()
                G_all_slices[batch_idx:batch_idx + batch_size] = g_out_np

            slice_offset = 10
            start_slice = 0
            end_slice = 150 if sh[0] >= 150 else sh[0]

            num_synthesized = 1 # WE ONLY SYNTHESIZE FLAIR

            rows = 4 + num_synthesized
            cols = (end_slice - start_slice) // slice_offset

            f, axes = plt.subplots(rows, cols, sharey=True, figsize=(30, 10))

            # plot the original real patient, row by row until 4th row
            # the order of sequences in HDF5 is as follows: (Not T1, T1c, T2, T2F).

            if dataset == 'ISLES2015':
                seq_names = ['T1', 'T2', 'DWI', 'T2F']
            else:
                seq_names = ['T1', 'T2', 'T1c', 'T2F']

            for curr_row in range(0, 4):  # curr_row also controls the sequence to show
                for curr_col, curr_slice_idx in zip(range(0, cols), range(start_slice, end_slice, slice_offset)):
                    # we use patient['image'] as it contains ALL slices for the patient, not batch wise
                    axes[curr_row, curr_col].imshow(np.clip(patient_numpy[curr_slice_idx, curr_row, ...] /
                                                            np.max(patient_numpy[curr_slice_idx, curr_row, ...]), 0.0, 1.0),
                                                    cmap='gray', vmin=0, vmax=1)
                    axes[curr_row, curr_col].axis('off')
                axes[curr_row, 0].axis('on')
                axes[curr_row, 0].get_xaxis().set_ticks([])
                axes[curr_row, 0].get_yaxis().set_ticks([])
                axes[curr_row, 0].set_ylabel(seq_names[curr_row], rotation=90, size='large')

            # now we plot the synthesized one
            indices = [0] # 3 is the index for T2F, but we only have 1 index coming out of generator, so 0.
            flair_index = 3
            for curr_row, curr_sequence_index in zip(range(4, rows),
                                                     indices):  # curr_row also controls the sequence to show
                for curr_col, curr_slice_idx in zip(range(0, cols), range(start_slice, end_slice, slice_offset)):
                    # we use patient['image'] as it contains ALL slices for the patient, not batch wise
                    axes[curr_row, curr_col].imshow(np.clip(G_all_slices[curr_slice_idx, curr_sequence_index,...] /
                                                            np.max(G_all_slices[curr_slice_idx, curr_sequence_index,...]), 0.0, 1.0),
                                                    cmap='gray', vmin=0, vmax=1)
                    axes[curr_row, curr_col].axis('off')
                axes[curr_row, 0].axis('on')
                axes[curr_row, 0].get_xaxis().set_ticks([])
                axes[curr_row, 0].get_yaxis().set_ticks([])
                axes[curr_row, 0].set_ylabel(seq_names[curr_sequence_index], rotation=90, size='large')

            # Question: Why is the ordering like this?
            # Answer: Check the original dataloader in dataloader.py. The "i" values are this way.
            if dataset == 'ISLES2015':
                plt.suptitle('Epoch: {}\nT1   T2   DWI   T2F\n{}   {}   {}   {}'.format(epoch,
                                                                                        curr_scenario[0],
                                                                                        curr_scenario[1],
                                                                                        curr_scenario[2],
                                                                                        curr_scenario[3]))
            else:
                plt.suptitle('Epoch: {}\nT1   T2   T1c   T2F\n{}   {}   {}   {}'.format(epoch,
                                                                                        curr_scenario[0],
                                                                                        curr_scenario[1],
                                                                                        curr_scenario[2],
                                                                                        curr_scenario[3]))

            f.text(0.65, 0.95, pat_name)
            f.text(0.35, 0.95, "MSE: %.5f" % np.mean(G_mse_loss))
            f.text(0.35, 0.935, "PSNR: %.5f" % np.mean(G_psnr_val))
            f.text(0.35, 0.92, "SSIM: %.5f" % np.mean(G_ssim_val))
            plt.subplots_adjust(wspace=0, hspace=0)
            plt.savefig(os.path.join(curr_saving_directory, "".join([str(x) for x in curr_scenario]) + ".png"))
            pylab.close(f)
        del patient_numpy, patient_copy, patient
    # put the generator back to train mode
    del G_all_slices
    return 0

# =========================================================================================
# Function used by original cDCGAN implementation that I based my code upon
# =========================================================================================
def show_result(num_epoch, show = False, save = False, path = 'result.png'):

    G.eval()
    test_images = G(fixed_z_, fixed_y_label_)
    G.train()

    size_figure_grid = 10
    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))
    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):
        ax[i, j].get_xaxis().set_visible(False)
        ax[i, j].get_yaxis().set_visible(False)

    for k in range(10*10):
        i = k // 10
        j = k % 10
        ax[i, j].cla()
        ax[i, j].imshow(test_images[k, 0].cpu().data.numpy(), cmap='gray')

    label = 'Epoch {0}'.format(num_epoch)
    fig.text(0.5, 0.04, label, ha='center')
    plt.savefig(path)

    if show:
        plt.show()
    else:
        plt.close()

def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):
    x = range(len(hist['D_losses']))

    y1 = hist['D_losses']
    y2 = hist['G_losses']

    plt.plot(x, y1, label='D_loss')
    plt.plot(x, y2, label='G_loss')

    plt.xlabel('Epoch')
    plt.ylabel('Loss')

    plt.legend(loc=4)
    plt.grid(True)
    plt.tight_layout()

    if save:
        plt.savefig(path)

    if show:
        plt.show()
    else:
        plt.close()


def create_dataloaders(parent_path='/scratch/asa224/asa224/Datasets/BRATS2018/HDF5_Datasets/',
                       parent_name='preprocessed',
                       dataset_name='training_data_hgg',
                       load_pat_names=True,
                       dataset_type='cropped',
                       load_seg=False,
                       transform_fn=[Resize(size=(256, 256)), ToTensor()],
                        apply_normalization = True,
                       which_normalization=None,
                       train_range=None,
                       resize_slices=148,
                       get_viz_dataloader=True,
                       num_workers=0,
                       dataset='BRATS2018',
                       load_indices=None,
                       shuffle=False):

    logger.info("Setting paths")
    # train_h5path = os.path.join(parent_path, 'BRATS_Combined.h5')
    if dataset == 'BRATS2018':
        if dataset_type == 'cropped':
            train_h5path = os.path.join(parent_path, 'BRATS2018_Cropped.h5')
        else:
            train_h5path = os.path.join(parent_path, 'BRATS2018.h5')

        if 'lgg' in dataset_name:
            mean_var_path = os.path.join(parent_path, 'BRATS2018_HDF5_Datasetstraining_data_lgg_mean_var.p')
        else:
            mean_var_path = os.path.join(parent_path, 'BRATS2018_HDF5_Datasetstraining_data_hgg_mean_var.p')


    elif dataset == 'BRATS2015':
        if dataset_type == 'cropped':
            train_h5path = os.path.join(parent_path, 'BRATS2015_Cropped.h5')
        else:
            train_h5path = os.path.join(parent_path, 'BRATS2015.h5')

        # Only use the LGG data as per the experiment design. It's actually mean and var, not std. Ignore the name
        mean_var_path = os.path.join(parent_path, 'HDF5_Datasetstraining_data_lgg_mean_std.p')

    elif dataset == 'ISLES2015':
        if dataset_type == 'cropped':
            train_h5path = os.path.join(parent_path, 'ISLES2015_Cropped.h5')
        else:
            train_h5path = os.path.join(parent_path, 'ISLES2015.h5')

        # only using SISS data. It's actually mean and var, not std. Ignore the name
        mean_var_path = os.path.join(parent_path, 'HDF5_Datasetstraining_data_mean_std.p')

    logger.debug('Using the following paths:')
    logger.debug('train_h5_path: {}'.format(train_h5path))
    logger.debug('mean_var_path: {}'.format(mean_var_path))

    transform = transforms.Compose(transform_fn)

    # build loader object
    logger.info("Build loader object")
    loader = BrainMRIData(train_h5path, mean_var_path, parent_name,
                          dataset_name, load_pat_names,
                          load_seg, transform, apply_normalization=apply_normalization,
                          which_normalization=which_normalization,
                          dataset=dataset, load_indices=load_indices, train_range=train_range)

    logger.info("Getting DataLoader instance using the loader object")
    # batch size here means patients. How many patients to load at once?
    dataloader = DataLoader(loader, batch_size=1,
                            shuffle=shuffle, num_workers=num_workers, collate_fn=my_collate)

    if get_viz_dataloader:
        dataloader_for_viz = loader
        return dataloader, dataloader_for_viz

    return dataloader


def impute_reals_into_fake(x_z, fake_x, label_scenario):
    for idx, k in enumerate(label_scenario):
        if k == 1:  # THIS IS A REAL AVAILABLE SEQUENCE
            fake_x[:, idx, ...] = x_z[:, idx, ...].clone().cuda()

    return fake_x


def save_checkpoint(state, filename, pickle_module):
    torch.save(state, filename, pickle_module=pickle_module)

def load_checkpoint(model, optimizer, filename, pickle_module):
    if os.path.isfile(filename):
        logger.info("Loading checkpoint '{}'".format(filename))
        checkpoint = torch.load(filename, pickle_module=pickle_module)
        # args.start_epoch = checkpoint['epoch']
        # best_prec1 = checkpoint['best_prec1']
        model.load_state_dict(checkpoint['state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer'])
        logger.info("Loaded checkpoint '{}' (epoch {})"
              .format(filename, checkpoint['epoch']))
    else:
        logger.critical('Checkpoint {} does not exist.'.format(filename))

    return model, optimizer



def psnr_torch(pred, gt):
    # normalize images between [0, 1]
    epsilon = 0.00001
    epsilon2 = torch.from_numpy(np.array(0.01, dtype=np.float32))
    # always use ground truth
    gt_n =     gt / (gt.max() + epsilon)
    pred_n = pred / (pred.max() + epsilon)

    PIXEL_MAX = 1.0

    mse = torch.mean((gt_n - pred_n) ** 2)
    if mse.item() == 0.0:
        psnr = 20 * torch.log10(PIXEL_MAX / epsilon2)
    else:
        psnr = 20 * torch.log10(PIXEL_MAX / torch.sqrt(mse))

    return psnr


def l2_torch(a, b):
    return torch.mean((a - b) ** 2)

def calculate_metrics(G, patient_list,
                          save_path, all_scenarios,
                          epoch, curr_scenario_range=None,
                          batch_size_to_test=2,
                          impute_type=None,
                          dataset = 'ISLES2015',
                          convert_normalization=False,
                            save_stats=False,
                          mean_var_file=None,
                      use_pytorch_ssim=False, seq_type='T1'):

    """
    For ISLES2015
    all_scenarios: scenarios
    curr_scenario_range: None
    batch_to_test: 2
    """

    if isinstance(patient_list, list):
        patients = patient_list
    else:
        patients = [patient_list]
    # put the generator in EVAL mode.
    mse = nn.MSELoss()
    save_im_path = os.path.join(save_path, 'all_slices', 'epoch_{}'.format(epoch))

    if not os.path.isdir(save_im_path):
        os.makedirs(save_im_path)


    # contains metrics for EACH slice from EACH OF THE SCENARIO. Basically everything. This is what we need for
    # ISLES2015
    running_mse = {}
    running_psnr = {}
    running_ssim = {}

    for (pat_ind, patient) in enumerate(patients):
        pat_name = patient['name'].decode('UTF-8')
        logger.debug('Testing Patient: {}'.format(pat_name))
        patient_image = patient['image']

        patient_copy = patient['image'].clone()

        patient_numpy = patient_copy.detach().cpu().numpy()

        scenarios = all_scenarios
        all_minus_1_g = torch.ones((batch_size_to_test,1,256,256)).cuda() * -1.0
        all_minus_x_test_r = torch.ones((batch_size_to_test, 256, 256)).cuda() * -1.0

        sh = patient_numpy.shape

        batch_size = batch_size_to_test

        # this will store output for ALL patients

        if curr_scenario_range is not None:
            scenarios = scenarios[curr_scenario_range[0]:curr_scenario_range[1]]

        logger.debug('Testing on scenarios: {}'.format(scenarios))
        for curr_scenario in scenarios:

            curr_scenario_str = ''.join([str(x) for x in curr_scenario])

            running_mse[curr_scenario_str] = []
            running_psnr[curr_scenario_str] = []
            running_ssim[curr_scenario_str] = []

            logger.debug('Testing on scenario: {}'.format(curr_scenario))

            # get the batch indices
            batch_indices = range(0, sh[0], batch_size)

            # for each batch
            for _num, batch_idx in enumerate(batch_indices):
                x_test_r = patient_image[batch_idx:batch_idx + batch_size, ...].cuda()
                x_test_z = x_test_r.clone().cuda().type(torch.cuda.FloatTensor)

                if impute_type == 'noise':
                    impute_tensor = torch.randn((batch_size,
                                                 256,
                                                 256), device='cuda')

                elif impute_type == 'average':
                    avail_indx = [i for i, x in enumerate(curr_scenario) if x == 1]
                    impute_tensor = torch.mean(x_test_r[:, avail_indx, ...], dim=1)
                elif impute_type == 'zeros':
                    impute_tensor = torch.zeros((batch_size,
                                                 256,
                                                 256), device='cuda')
                else:
                    impute_tensor = torch.ones((sh[-2], sh[-1])) * -1.0
                    # print('Imputing with -1')

                # print('Imputing with {}'.format(impute_type))
                for idx_, k in enumerate(curr_scenario):
                    if k == 0:
                        x_test_z[:, idx_, ...] = impute_tensor

                G_result = G(x_test_z)
                # save all images
                if dataset == 'ISLES2015' or dataset == 'BRATS2015':
                    if 'ISLES' in dataset:
                        SEQ_IND = 3
                        ssim = compare_ssim
                    else:
                        if seq_type == "T1":
                            SEQ_IND = 0
                        elif seq_type == "T2":
                            SEQ_IND = 1
                        ssim = pyt_ssim.ssim

                    if dataset == 'BRATS2015':

                        # den_G_result = G_result.max() - G_result.min()
                        # if den_G_result != 0.0:
                        #     G_result_norm = 2*((G_result - G_result.min())/ (den_G_result)) -1
                        # else:
                        #     G_result_norm = all_minus_1_g
                        #
                        # den_x_test_r = x_test_r[:, SEQ_IND, ...].max() - x_test_r[:, SEQ_IND, ...].min()
                        # if den_x_test_r != 0.0:
                        #     x_test_r_norm = 2*((x_test_r[:, SEQ_IND, ...] - x_test_r[:, SEQ_IND, ...].min())/ (den_x_test_r)) - 1
                        # else:
                        #     x_test_r_norm = all_minus_x_test

                        # calculate metrics
                        running_mse[curr_scenario_str].append(
                            mse(G_result,
                                x_test_r[:, SEQ_IND, ...].unsqueeze(1)).item())

                        running_ssim[curr_scenario_str].append(ssim(
                            G_result[:, 0].unsqueeze(1) / (torch.max(x_test_r[:, SEQ_IND]) + 0.0001),
                            x_test_r[:, SEQ_IND].unsqueeze(1) / (torch.max(x_test_r[:, SEQ_IND]) + 0.0001),
                            val_range=1).item())

                        running_psnr[curr_scenario_str].append(
                            psnr_torch(G_result, x_test_r[:, SEQ_IND, ...].unsqueeze(1)).item())

                        # running_mse[curr_scenario_str].append(
                        #     mse(G_result, x_test_r[:,SEQ_IND].unsqueeze(1)).item())
                        #
                        # s = ssim(G_result, x_test_r[:,SEQ_IND].unsqueeze(1), val_range=2).item()
                        # if s > 0:
                        #     running_ssim[curr_scenario_str].append(s)
                        # else:
                        #     running_ssim[curr_scenario_str].append(0.0)
                        #
                        # p = psnr_torch(G_result, x_test_r[:, SEQ_IND].unsqueeze(1), val_range=2).item()
                        # if p > 0:
                        #     running_psnr[curr_scenario_str].append(p)
                        # else:
                        #     running_psnr[curr_scenario_str].append(0)

                        real_filepath = os.path.join(save_im_path, '{}-{}_real.png'.format(pat_ind, _num))
                        fake_filepath = os.path.join(save_im_path, '{}-{}_fake.png'.format(pat_ind, _num))

                        scipy.misc.toimage(G_result[0, 0].detach().cpu().numpy(), cmin=-1.0, cmax=1.0).save(
                            fake_filepath)
                        scipy.misc.toimage(x_test_r[0, SEQ_IND].detach().cpu().numpy(), cmin=-1.0, cmax=1.0).save(
                            real_filepath)

                    else:
                        running_mse[curr_scenario_str].append(
                            mse(G_result,
                                x_test_r[:, SEQ_IND, ...].unsqueeze(1)).item())

                        running_ssim[curr_scenario_str].append(ssim(
                            G_result[:, 0].unsqueeze(1) / (torch.max(x_test_r[:, SEQ_IND]) + 0.0001),
                            x_test_r[:, SEQ_IND].unsqueeze(1) / (torch.max(x_test_r[:, SEQ_IND]) + 0.0001),
                            val_range=1).item())

                        # running_ssim[curr_scenario_str].append(ssim(G_result[:, 0].unsqueeze(1),
                        #                                                     x_test_r[:, SEQ_IND].unsqueeze(1)).item())

                        running_psnr[curr_scenario_str].append(
                            psnr_torch(G_result, x_test_r[:, SEQ_IND, ...].unsqueeze(1)).item())

                else:
                    for idx_curr_label, j in enumerate(curr_scenario):
                        if j == 0:
                            running_mse[curr_scenario_str].append(
                                mse(G_result[:, idx_curr_label] /
                                    (torch.max(G_result[:, idx_curr_label]) + 0.0001),
                                    x_test_r[:, idx_curr_label] /
                                    (torch.max(x_test_r[:, idx_curr_label]) + 0.0001)).item())

                            running_ssim[curr_scenario_str].append(pyt_ssim.ssim(
                                G_result[:, idx_curr_label].unsqueeze(1) /
                                (torch.max(G_result[:, idx_curr_label]) + 0.0001),
                                x_test_r[:, idx_curr_label].unsqueeze(1) /
                                (torch.max(x_test_r[:, idx_curr_label]) + 0.0001),
                                val_range=1).item())

                            running_psnr[curr_scenario_str].append(
                                psnr_torch(G_result[:, idx_curr_label],
                                           x_test_r[:, idx_curr_label]).item())



    num_dict = {}
    all_mean_mse = []
    all_mean_psnr = []
    all_mean_ssim = []

    for (mse_key, mse_list, psnr_key, psnr_list, ssim_key, ssim_list) in zip(running_mse.keys(), running_mse.values(),
                                                                             running_psnr.keys(), running_psnr.values(),
                                                                             running_ssim.keys(), running_ssim.values()):

        assert mse_key == ssim_key == psnr_key
        num_dict[mse_key] = {
            'mse': np.mean(mse_list),
            'psnr': np.mean(psnr_list),
            'ssim': np.mean(ssim_list)
        }

        all_mean_mse += mse_list
        all_mean_psnr += psnr_list
        all_mean_ssim += ssim_list

    num_dict['mean'] = {
        'mse': np.mean(all_mean_mse),
        'psnr': np.mean(all_mean_psnr),
        'ssim': np.mean(all_mean_ssim)
    }
    if save_stats:
        stat_folder = os.path.join(save_path, "stats/".format())
        if not os.path.isdir(stat_folder):
            os.makedirs(stat_folder)
        print('Saving running statistics to folder: {}'.format(stat_folder))
        # save mse, psnr and ssim
        pickle.dump(running_mse, open(os.path.join(stat_folder, 'mse.p'), 'wb'))
        pickle.dump(running_psnr, open(os.path.join(stat_folder, 'psnr.p'), 'wb'))
        pickle.dump(running_ssim, open(os.path.join(stat_folder, 'ssim.p'), 'wb'))

        return num_dict, running_mse, running_psnr, running_ssim

    return num_dict


def calculate_metrics_pgan(G, patient_list,
                          save_path, all_scenarios,
                          epoch, curr_scenario_range=None,
                          batch_size_to_test=2,
                          dataset = 'ISLES2015',
                          convert_normalization=False,
                          mean_var_file=None,
                      use_pytorch_ssim=False, seq_type='T1'):

    """
    For ISLES2015
    all_scenarios: scenarios
    curr_scenario_range: None
    batch_to_test: 2
    """

    if isinstance(patient_list, list):
        patients = patient_list
    else:
        patients = [patient_list]
    # put the generator in EVAL mode.
    mse = nn.MSELoss()

    # contains metrics for EACH slice from EACH OF THE SCENARIO. Basically everything. This is what we need for
    # ISLES2015
    running_mse = []
    running_psnr = []
    running_ssim = []

    for (pat_ind, patient) in enumerate(patients):
        pat_name = patient['name'].decode('UTF-8')
        logger.debug('Testing Patient: {}'.format(pat_name))
        patient_image = patient['image']

        patient_copy = patient['image'].clone()

        patient_numpy = patient_copy.detach().cpu().numpy()

        sh = patient_numpy.shape

        batch_size = batch_size_to_test

        # get the batch indices
        batch_indices = range(0, sh[0], batch_size)

        if seq_type == 'T1':
            SEQ_IDX = 1  # WE TRAIN WITH T2
            SEQ_IDX_SYNTH = 0
        else:
            SEQ_IDX = 0  # WE TRAIN WITH T1
            SEQ_IDX_SYNTH = 1

        # for each batch
        for _num, batch_idx in enumerate(batch_indices):
            x_test_r = patient_image[batch_idx:batch_idx + batch_size, SEQ_IDX_SYNTH,...].unsqueeze(1).cuda()
            x_test_z =patient_image[batch_idx:batch_idx + batch_size, SEQ_IDX,...].unsqueeze(1).cuda()

            G_result = G(x_test_z)
            ssim = pyt_ssim.ssim

            running_mse.append(
                mse(G_result,
                    x_test_r).item())

            running_ssim.append(ssim(
                G_result / (torch.max(x_test_r) + 0.0001),
                x_test_r / (torch.max(x_test_r) + 0.0001),
                val_range=1).item())

            # running_ssim[curr_scenario_str].append(ssim(G_result[:, 0].unsqueeze(1),
            #                                                     x_test_r[:, SEQ_IND].unsqueeze(1)).item())

            running_psnr.append(
                psnr_torch(G_result, x_test_r).item())

    num_dict = {}
    num_dict['mean'] = {
        'mse': np.mean(running_mse),
        'psnr': np.mean(running_psnr),
        'ssim': np.mean(running_ssim)
    }

    return num_dict


def printTable(result_dict):
    print("{:<10} {:<10} {:<10} {:<10}".format('Scenario', 'MSE', 'PSNR', 'SSIM'))
    result_keys = sorted(result_dict.keys())
    for k, v in zip(result_keys, result_dict.values()):
        print("{:<10} {:<10.4f} {:<10.4f} {:<10.4f}".format(k, result_dict[k]['mse'],
                                                            result_dict[k]['psnr'],
                                                            result_dict[k]['ssim']))
